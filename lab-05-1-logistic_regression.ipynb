{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 초기 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1040739f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. training set과 Variable 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = np.array([[1, 2],\n",
    "                   [2, 3],\n",
    "                   [3, 1],\n",
    "                   [4, 3],\n",
    "                   [5, 3],\n",
    "                   [6, 2]], dtype=np.float32)\n",
    "y_data = np.array([[0],\n",
    "                   [0],\n",
    "                   [0],\n",
    "                   [1],\n",
    "                   [1],\n",
    "                   [1]], dtype=np.float32)\n",
    "\n",
    "X = Variable(torch.from_numpy(x_data))\n",
    "Y = Variable(torch.from_numpy(y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드를 짜면서 느끼는 거지만 이 단계에서 사용하는 함수들은 tensorflow의 node 같은 느낌이다. \n",
    "# 5단계에서 Variable을 넣어줘야만 움직이고 그냥 아래처럼 대입만해서는 동작하지 않기 때문이다.\n",
    "# 따라서 밑에서 cost function은 선언하지 않는데,\n",
    "# logistic regression의 cost는 특정 선언해야 할 함수 없이 실제 훈련 시에 직접 계산하기 때문이다.\n",
    "\n",
    "hypothesis = torch.nn.Linear(2, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "# torch.nn.Sequential은 위에 선언한 두 node를 합쳐준다. parameter 순서가 중요하다.\n",
    "model = torch.nn.Sequential(hypothesis, sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. cost function과 optimizer 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logistic regression을 훈련시키기 위해서는 gradient descent algorithm을 사용한다.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. model 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 1.71229613]\n",
      "200 [ 0.55874169]\n",
      "400 [ 0.48905891]\n",
      "600 [ 0.45216355]\n",
      "800 [ 0.42799503]\n",
      "1000 [ 0.40953714]\n",
      "1200 [ 0.39402437]\n",
      "1400 [ 0.38024762]\n",
      "1600 [ 0.36763126]\n",
      "1800 [ 0.35588053]\n",
      "2000 [ 0.34483218]\n",
      "2200 [ 0.33438912]\n",
      "2400 [ 0.32448757]\n",
      "2600 [ 0.31508172]\n",
      "2800 [ 0.30613571]\n",
      "3000 [ 0.29761967]\n",
      "3200 [ 0.28950715]\n",
      "3400 [ 0.28177419]\n",
      "3600 [ 0.27439904]\n",
      "3800 [ 0.2673611]\n",
      "4000 [ 0.2606411]\n",
      "4200 [ 0.25422108]\n",
      "4400 [ 0.24808392]\n",
      "4600 [ 0.24221388]\n",
      "4800 [ 0.23659591]\n",
      "5000 [ 0.23121582]\n",
      "5200 [ 0.2260606]\n",
      "5400 [ 0.22111771]\n",
      "5600 [ 0.21637559]\n",
      "5800 [ 0.21182325]\n",
      "6000 [ 0.2074506]\n",
      "6200 [ 0.20324801]\n",
      "6400 [ 0.1992064]\n",
      "6600 [ 0.1953174]\n",
      "6800 [ 0.19157311]\n",
      "7000 [ 0.18796612]\n",
      "7200 [ 0.18448943]\n",
      "7400 [ 0.18113652]\n",
      "7600 [ 0.17790125]\n",
      "7800 [ 0.17477784]\n",
      "8000 [ 0.17176092]\n",
      "8200 [ 0.16884524]\n",
      "8400 [ 0.16602618]\n",
      "8600 [ 0.16329901]\n",
      "8800 [ 0.16065963]\n",
      "9000 [ 0.15810403]\n",
      "9200 [ 0.1556284]\n",
      "9400 [ 0.15322906]\n",
      "9600 [ 0.15090273]\n",
      "9800 [ 0.14864622]\n",
      "10000 [ 0.14645647]\n"
     ]
    }
   ],
   "source": [
    "for step in range(10001):\n",
    "    # optimizer를 초기화한다.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    Y_hat = model(X)\n",
    "    # Variable은 계산해도 Variable이다. 그래서 cost도 Variable.\n",
    "    cost = -(Y * torch.log(Y_hat) +\n",
    "             (1 - Y) * torch.log(1 - Y_hat)).mean()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        print(step, cost.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. model 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correct (Y): [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]] \n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "predicted = (model(X).data > 0.5).float()\n",
    "accuracy = (predicted == Y.data).float().mean()\n",
    "print(\"\\nCorrect (Y):\", predicted.numpy(), \"\\nAccuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variable의 required_grad는 기본값이 False인데,\n",
    "# 이 의미는 이 Variable은 Backpropagation시에 변화량을 안 받겠다는 것이다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
